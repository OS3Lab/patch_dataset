--- a/mm/internal.h
+++ b/mm/internal.h
@@ -652,6 +652,10 @@
 	return fpin;
 }
 #else /* !CONFIG_MMU */
+static inline int unmap_poisoned_folio(struct folio *folio, unsigned long pfn, bool must_kill)
+{
+	return -EBUSY;
+}
 static inline void unmap_mapping_folio(struct folio *folio) { }
 static inline void mlock_vma_page(struct page *page,
 			struct vm_area_struct *vma, bool compound) { }

--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -1482,6 +1482,7 @@
 			try_to_unmap(folio, ttu|TTU_RMAP_LOCKED);
 			i_mmap_unlock_write(mapping);
 		} else
 			pr_info("%#lx: could not lock mapping for mapped huge page\n", pfn);
+			return -EBUSY;
 	} else {
 		try_to_unmap(folio, ttu);

--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -1415,8 +1415,6 @@ static bool hwpoison_user_mappings(struct page *p, unsigned long pfn,
 				  int flags, struct page *hpage)
 {
 	struct folio *folio = page_folio(hpage);
-	enum ttu_flags ttu = TTU_IGNORE_MLOCK | TTU_SYNC | TTU_HWPOISON;
-	struct address_space *mapping;
 	LIST_HEAD(tokill);
 	bool unmap_success;
 	int forcekill;

--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -1434,6 +1434,29 @@ static bool hwpoison_user_mappings(struct page *p, unsigned long pfn,
 	if (!page_mapped(p))
 		return true;
 
+if (PageSwapCache(p)) {
+	pr_err("%#lx: keeping poisoned page in swap cache\n", pfn);
+	ttu &= ~TTU_HWPOISON;
+}
+
+/*
+ * Propagate the dirty bit from PTEs to struct page first, because we
+ * need this to decide if we should kill or just drop the page.
+ * XXX: the dirty test could be racy: set_page_dirty() may not always
+ * be called inside page lock (it->recommended but not enforced).
+ */
+mapping = page_mapping(hpage);
+if (!(flags & MF_MUST_KILL) && !PageDirty(hpage) && mapping &&
+    mapping_can_writeback(mapping)) {
+	if (page_mkclean(hpage)) {
+		SetPageDirty(hpage);
+	} else {
+		ttu &= ~TTU_HWPOISON;
+		pr_info("%#lx: corrupted page was clean: dropped without side effects\n",
+			pfn);
+	}
+}
+
 	if (PageKsm(p)) {
 		pr_err("%#lx: can't handle KSM pages.\n", pfn);
 		return false;

--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -1485,7 +1485,7 @@
 		try_to_unmap(folio, ttu);
 	}
 
-	unmap_success = !page_mapped(p);
+unmap_success = !try_to_unmap(folio, ttu);
 	if (!unmap_success)
 		pr_err("%#lx: failed to unmap page (mapcount=%d)\n",
 		       pfn, page_mapcount(p));

--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -1659,6 +1659,7 @@ static void do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)
 			if (WARN_ON(folio_test_lru(folio)))
 				folio_isolate_lru(folio);
 			if (folio_mapped(folio))
-				try_to_unmap(folio, TTU_IGNORE_MLOCK);
+				unmap_poisoned_folio(folio, pfn, false);
+
 			continue;
 		}
